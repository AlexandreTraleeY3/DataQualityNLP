{"cells":[{"cell_type":"markdown","metadata":{"id":"9D7wRnQKkQji"},"source":["# QQP"]},{"cell_type":"markdown","source":["## Preparing the Environment"],"metadata":{"id":"y0PRsh9tuGEn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3M_cf9VSVMB"},"outputs":[],"source":["#Google Colab - Drive Mounting\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTO-luPHQyuS"},"outputs":[],"source":["#Install missing library keras-nlp\n","!pip install -q keras-nlp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ub9od90CSWjP"},"outputs":[],"source":["#Import the libraries\n","import tensorflow as tf\n","import keras_nlp\n","from tensorflow import keras\n","import numpy as np\n","import os\n","import re\n","import pandas as pd\n","import string\n","import random"]},{"cell_type":"markdown","source":["## Data Preprocessing and Parameters Initialization"],"metadata":{"id":"gKTzZOdCujuY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyWWpDCVSYJn"},"outputs":[],"source":["#Finetuning Parameters\n","FINETUNING_LEARNING_RATE = 5e-5\n","FINETUNING_EPOCHS = 3\n","FINETUNING_BATCH_SIZE = 32\n","SEQ_LENGTH = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fY7vRlAoTJtB"},"outputs":[],"source":["# Download of the vocabulary from BERT: Bert-uncased\n","vocab_file = keras.utils.get_file(\n","    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",")\n","#Initialization of the Word Tokenizer, with a given vocabulary and a sequence length\n","tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n","    vocabulary=vocab_file, sequence_length=SEQ_LENGTH,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYuyh5eUSlYx"},"outputs":[],"source":["# Load QQP train/validation/test\n","qqp_train_ds = tf.data.experimental.CsvDataset(\n","    \"path_to_GLUE/QQP/\" + \"train.tsv\", [tf.int32, tf.int32, tf.int32, tf.string, tf.string,tf.int32], header=True, field_delim=\"\\t\"\n",").batch(FINETUNING_BATCH_SIZE)\n","qqp_val_ds = tf.data.experimental.CsvDataset(\n","    \"path_to_GLUE/QQP/\" + \"dev.tsv\", [tf.int32, tf.int32, tf.int32, tf.string, tf.string,tf.int32], header=True, field_delim=\"\\t\"\n",").batch(FINETUNING_BATCH_SIZE)\n","qqp_test_ds = tf.data.experimental.CsvDataset(\n","    \"path_to_GLUE/QQP/\" + \"test.tsv\", [tf.int32, tf.string, tf.string], header=True, field_delim=\"\\t\"\n",").batch(FINETUNING_BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4C3I8J2VqVe"},"outputs":[],"source":["def preprocess(index,src1,src2,sentence1,sentence2,label):\n","  \"\"\"\n","  Given two sentences, return the two sentences combined with a separator and the label\n","  \"\"\"\n","  text = sentence1 + '[SEP]' + sentence2\n","  return tokenizer(text),label\n","\n","\n","#Map the Data to the preprocess function\n","finetune_ds = qqp_train_ds.map(\n","    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",").prefetch(tf.data.AUTOTUNE)\n","finetune_val_ds = qqp_val_ds.map(\n","    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",").prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVDICRarTNx2"},"outputs":[],"source":["# Load the pretrained model and display its structure\n","model = keras.models.load_model('path_to_the_pretrained_model',compile=False)\n","model.summary()"]},{"cell_type":"code","source":["# The model's head is modified for classification\n","\n","inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n","encoded_tokens = model(inputs)\n","x = keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n","x = keras.layers.Dropout(0.1)(x)\n","x = keras.layers.Dense(768, activation=\"tanh\")(x)\n","x = keras.layers.Dropout(0.1)(x)\n","outputs = keras.layers.Dense(2, activation=\"softmax\")(x)\n","\n","finetuning_model = keras.models.Model(inputs=inputs, outputs=outputs)\n","finetuning_model.summary()"],"metadata":{"id":"oZ_6eicfvUyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training"],"metadata":{"id":"biVHQSlJxa6N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7w3N-TXfWmdK"},"outputs":[],"source":["#Create tensorboard callback\n","logdir = \"path_to_save_execution_information\" #+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n","\n","#Compile model \n","finetuning_model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=FINETUNING_LEARNING_RATE),\n","    metrics=[\"sparse_categorical_accuracy\"],\n",")"]},{"cell_type":"code","source":["#Model training\n","finetuning_model.fit(\n","    finetune_ds, \n","    validation_data=finetune_val_ds, \n","    epochs=FINETUNING_EPOCHS,\n","    callbacks=tensorboard_callback)"],"metadata":{"id":"FKAQRO-Jxp5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXGa32MjZrkH"},"outputs":[],"source":["# Add Tokenization layer to the model\n","inputs = keras.Input(shape=(), dtype=tf.string)\n","tokens = tokenizer(inputs)\n","outputs = finetuning_model(tokens)\n","\n","#Save model\n","final_model = keras.Model(inputs, outputs)\n","final_model.save(\"path_to_save_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7_YRZqwPCKy"},"outputs":[],"source":["# Restore the saved model\n","restored_model = keras.models.load_model(\"path_to_save_model\", compile=False)"]},{"cell_type":"markdown","source":["## Testing"],"metadata":{"id":"TiABd972yBYv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JYouTzwLfl-"},"outputs":[],"source":["def preprocess_test(index,sentence1,sentence2):\n","  \"\"\"\n","  Given two sentences, return the two sentences combined with a separator\n","  \"\"\"\n","  text = sentence1 + '[SEP]' + sentence2\n","  return tokenizer(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5aDgSe2O1G2"},"outputs":[],"source":["# Load and generate test data\n","finetune_test_ds = sst_test_ds.map(\n","    preprocess_test, num_parallel_calls=tf.data.AUTOTUNE\n",").prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmWjBHrQPM-N"},"outputs":[],"source":["#Generate predictions\n","result = fine_model.predict(finetune_test_ds, batch_size=128,verbose=True).argmax(axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-CfbvQCPQm5"},"outputs":[],"source":["#Format results into dataframe, ready to be uploaded on gluebenchmark.com\n","df = pd.DataFrame(result)\n","\n","df.to_csv(\"QQP.tsv\",sep='\\t', encoding='utf-8')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PAQvGVUWqHC"},"outputs":[],"source":["#Load Tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir=\"path_to_save_execution_information\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQS8NwvrrfAz"},"outputs":[],"source":["#Code to automatically stop the run time for Google Colab\n","import time\n","time.sleep(60)\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
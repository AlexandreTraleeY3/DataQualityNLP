{"cells":[{"cell_type":"markdown","source":["# Sequence2Sequence Model"],"metadata":{"id":"AYCTTgtN-CnB"}},{"cell_type":"markdown","source":["## Preparing the Environment\n"],"metadata":{"id":"0FRw5fe6-LW3"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64750,"status":"ok","timestamp":1669039818369,"user":{"displayName":"NLP Master","userId":"02125616247832443062"},"user_tz":-60},"id":"s2iLJSIih9x5","outputId":"dfced057-c431-4e9b-e124-08fab008197a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["#Google Colab - Drive Mounting\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118544,"status":"ok","timestamp":1669039936907,"user":{"displayName":"NLP Master","userId":"02125616247832443062"},"user_tz":-60},"id":"Rxmq62AXiMiN","outputId":"85d74359-e3e7-4855-8922-9d7818385501"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 151 kB 14.0 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 70.4 MB/s \n","\u001b[K     |████████████████████████████████| 588.3 MB 7.1 kB/s \n","\u001b[K     |████████████████████████████████| 5.9 MB 56.2 MB/s \n","\u001b[K     |████████████████████████████████| 578.1 MB 30 kB/s \n","\u001b[K     |████████████████████████████████| 578.0 MB 20 kB/s \n","\u001b[K     |████████████████████████████████| 4.6 MB 52.7 MB/s \n","\u001b[?25h"]}],"source":["#Install missing library keras-nlp\n","!pip install -q keras-nlp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AuthuT8l6H5"},"outputs":[],"source":["# Import the libraries\n","import os\n","import keras_nlp\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"]},{"cell_type":"markdown","source":["## Data Preprocessing and Parameters Initialization"],"metadata":{"id":"cbgkJnUd-Ya2"}},{"cell_type":"markdown","source":["Reference: https://keras.io/guides/keras_nlp/transformer_pretraining/"],"metadata":{"id":"DTr_Rpq6Arl5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1669039939902,"user":{"displayName":"NLP Master","userId":"02125616247832443062"},"user_tz":-60},"id":"q5-GSbDvmPVx","outputId":"3a58da64-1fb4-484f-fbc9-11f7fdad353f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\n","231508/231508 [==============================] - 0s 1us/step\n"]}],"source":["# Download of the vocabulary from BERT: Bert-uncased\n","vocab_file = keras.utils.get_file(\n","    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",")\n","#Initialization of the Word Tokenizer, with a given vocabulary and a sequence length\n","tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n","    vocabulary=vocab_file, sequence_length=SEQ_LENGTH,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdUiooQHmvFp"},"outputs":[],"source":["# Preprocessing paramseters\n","PRETRAINING_BATCH_SIZE = 128\n","FINETUNING_BATCH_SIZE = 32\n","SEQ_LENGTH = 128\n","MASK_RATE = 0.25\n","PREDICTIONS_PER_SEQ = 32\n","\n","# Model paramerters\n","NUM_LAYERS = 2\n","MODEL_DIM = 256\n","INTERMEDIATE_DIM = 512\n","NUM_HEADS = 4\n","DROPOUT = 0.1\n","NORM_EPSILON = 1e-5\n","\n","# Training paramseters\n","PRETRAINING_LEARNING_RATE = 5e-4\n","PRETRAINING_EPOCHS = 15 \n","FINETUNING_LEARNING_RATE = 5e-5\n","FINETUNING_EPOCHS = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAqYPidH39Nn"},"outputs":[],"source":["# Load the data\n","gutenberg_train_ds = (\n","    tf.data.TextLineDataset('path_to_pretraining_train_data/train-all.csv')\n","    .filter(lambda x: tf.strings.length(x) > 100)\n","    .batch(PRETRAINING_BATCH_SIZE)\n",")\n","gutenberg_val_ds = (\n","    tf.data.TextLineDataset('path_to_pretraining_test_data/test-all.csv')\n","    .filter(lambda x: tf.strings.length(x) > 100)\n","    .batch(PRETRAINING_BATCH_SIZE)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1esZpRO4Af-"},"outputs":[],"source":["def preprocess(inputs):\n","    \"\"\"\n","    Given a text input, \n","    return a set of features containing the masked words, \n","    the true labels of these masked word and their weights\n","    \"\"\"\n","    inputs = tokenizer(inputs)\n","    outputs = masker(inputs)\n","    features = {\n","        \"tokens\": outputs[\"tokens\"],\n","        \"mask_positions\": outputs[\"mask_positions\"],\n","    }\n","    labels = outputs[\"mask_ids\"]\n","    weights = outputs[\"mask_weights\"]\n","    return features, labels, weights\n","\n","# Create the Masker object\n","masker = keras_nlp.layers.MLMMaskGenerator(\n","    vocabulary_size=tokenizer.vocabulary_size(),\n","    mask_selection_rate=MASK_RATE,\n","    mask_selection_length=PREDICTIONS_PER_SEQ,\n","    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",")\n","\n","\n","# Preprocess the data\n","pretrain_ds = gutenberg_train_ds.map(\n","    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",").prefetch(tf.data.AUTOTUNE)\n","pretrain_val_ds = gutenberg_val_ds.map(\n","    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",").prefetch(tf.data.AUTOTUNE)\n","\n","# Display a single element of the dataset\n","print(pretrain_ds.take(1).get_single_element())"]},{"cell_type":"markdown","source":["## Build Model"],"metadata":{"id":"MgUI81Q3_pbL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":858,"status":"ok","timestamp":1669039956557,"user":{"displayName":"NLP Master","userId":"02125616247832443062"},"user_tz":-60},"id":"23Qsmqkd5GvP","outputId":"3bcdd0fe-fea9-4fe4-eca5-52c4ec2cf91a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 128)]             0         \n","                                                                 \n"," token_and_position_embeddin  (None, 128, 256)         7846400   \n"," g (TokenAndPositionEmbeddin                                     \n"," g)                                                              \n","                                                                 \n"," layer_normalization (LayerN  (None, 128, 256)         512       \n"," ormalization)                                                   \n","                                                                 \n"," dropout (Dropout)           (None, 128, 256)          0         \n","                                                                 \n"," transformer_encoder (Transf  (None, 128, 256)         527104    \n"," ormerEncoder)                                                   \n","                                                                 \n"," transformer_encoder_1 (Tran  (None, 128, 256)         527104    \n"," sformerEncoder)                                                 \n","                                                                 \n"," transformer_decoder (Transf  (None, 128, 256)         527104    \n"," ormerDecoder)                                                   \n","                                                                 \n"," transformer_decoder_1 (Tran  (None, 128, 256)         527104    \n"," sformerDecoder)                                                 \n","                                                                 \n","=================================================================\n","Total params: 9,955,328\n","Trainable params: 9,955,328\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Define input type\n","inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n","\n","# Embdding layer\n","embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n","    vocabulary_size=tokenizer.vocabulary_size(),\n","    sequence_length=SEQ_LENGTH,\n","    embedding_dim=MODEL_DIM,\n",")\n","outputs = embedding_layer(inputs)\n","\n","# Normalization and Dropout\n","outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n","outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n","\n","# Create encoder blocks\n","for i in range(NUM_LAYERS):\n","    outputs = keras_nlp.layers.TransformerEncoder(\n","        intermediate_dim=INTERMEDIATE_DIM,\n","        num_heads=NUM_HEADS,\n","        dropout=DROPOUT,\n","        layer_norm_epsilon=NORM_EPSILON,\n","    )(outputs)\n","\n","# Create decoder blocks\n","for i in range(NUM_LAYERS):\n","    outputs = keras_nlp.layers.TransformerDecoder(\n","        intermediate_dim=INTERMEDIATE_DIM,\n","        num_heads=NUM_HEADS,\n","        dropout=DROPOUT,\n","        layer_norm_epsilon=NORM_EPSILON,\n","    )(outputs)\n","\n","\n","#Build model and display\n","encoder_model = keras.Model(inputs, outputs)\n","encoder_model.summary()"]},{"cell_type":"markdown","source":["## Model Training"],"metadata":{"id":"nMHNVpvX_meZ"}},{"cell_type":"code","source":["# Adding masked language model head\n","inputs = {\n","    \"tokens\": keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32),\n","    \"mask_positions\": keras.Input(shape=(PREDICTIONS_PER_SEQ,), dtype=tf.int32),\n","}\n","\n","#Create tensorboard callback\n","logdir = \"path_to_save_execution_information\" #+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n","\n","# Encode the input tokens\n","encoded_tokens = encoder_model(inputs[\"tokens\"])\n","\n","# Predict output for each masked word\n","outputs = keras_nlp.layers.MLMHead(\n","    embedding_weights=embedding_layer.token_embedding.embeddings, activation=\"softmax\",\n",")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n","\n","# Compile our model\n","pretraining_model = keras.Model(inputs, outputs)\n","pretraining_model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=PRETRAINING_LEARNING_RATE),\n","    weighted_metrics=[\"sparse_categorical_accuracy\"],\n","    jit_compile=True,\n",")"],"metadata":{"id":"gaeGeSLr95Cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDx1u73w5fTu","executionInfo":{"status":"ok","timestamp":1669047341558,"user_tz":-60,"elapsed":7385006,"user":{"displayName":"NLP Master","userId":"02125616247832443062"}},"outputId":"4ad9f136-929d-4272-ad89-f1c2ac826fd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","2074/2074 [==============================] - 489s 227ms/step - loss: 2.8612 - sparse_categorical_accuracy: 0.1726 - val_loss: 2.5227 - val_sparse_categorical_accuracy: 0.2676\n","Epoch 2/15\n","2074/2074 [==============================] - 471s 227ms/step - loss: 2.2215 - sparse_categorical_accuracy: 0.2983 - val_loss: 2.2563 - val_sparse_categorical_accuracy: 0.3194\n","Epoch 3/15\n","2074/2074 [==============================] - 468s 226ms/step - loss: 2.0436 - sparse_categorical_accuracy: 0.3348 - val_loss: 2.1412 - val_sparse_categorical_accuracy: 0.3442\n","Epoch 4/15\n","2074/2074 [==============================] - 493s 238ms/step - loss: 1.9477 - sparse_categorical_accuracy: 0.3550 - val_loss: 2.0689 - val_sparse_categorical_accuracy: 0.3583\n","Epoch 5/15\n","2074/2074 [==============================] - 468s 226ms/step - loss: 1.8831 - sparse_categorical_accuracy: 0.3691 - val_loss: 2.0207 - val_sparse_categorical_accuracy: 0.3701\n","Epoch 6/15\n","2074/2074 [==============================] - 495s 239ms/step - loss: 1.8355 - sparse_categorical_accuracy: 0.3789 - val_loss: 1.9815 - val_sparse_categorical_accuracy: 0.3777\n","Epoch 7/15\n","2074/2074 [==============================] - 492s 237ms/step - loss: 1.7995 - sparse_categorical_accuracy: 0.3866 - val_loss: 1.9507 - val_sparse_categorical_accuracy: 0.3830\n","Epoch 8/15\n","2074/2074 [==============================] - 495s 239ms/step - loss: 1.7710 - sparse_categorical_accuracy: 0.3930 - val_loss: 1.9259 - val_sparse_categorical_accuracy: 0.3891\n","Epoch 9/15\n","2074/2074 [==============================] - 471s 227ms/step - loss: 1.7466 - sparse_categorical_accuracy: 0.3980 - val_loss: 1.9049 - val_sparse_categorical_accuracy: 0.3942\n","Epoch 10/15\n","2074/2074 [==============================] - 499s 241ms/step - loss: 1.7254 - sparse_categorical_accuracy: 0.4021 - val_loss: 1.8860 - val_sparse_categorical_accuracy: 0.3980\n","Epoch 11/15\n","2074/2074 [==============================] - 474s 229ms/step - loss: 1.7080 - sparse_categorical_accuracy: 0.4061 - val_loss: 1.8753 - val_sparse_categorical_accuracy: 0.4002\n","Epoch 12/15\n","2074/2074 [==============================] - 500s 241ms/step - loss: 1.6908 - sparse_categorical_accuracy: 0.4098 - val_loss: 1.8603 - val_sparse_categorical_accuracy: 0.4041\n","Epoch 13/15\n","2074/2074 [==============================] - 474s 229ms/step - loss: 1.6779 - sparse_categorical_accuracy: 0.4122 - val_loss: 1.8522 - val_sparse_categorical_accuracy: 0.4052\n","Epoch 14/15\n","2074/2074 [==============================] - 474s 229ms/step - loss: 1.6669 - sparse_categorical_accuracy: 0.4150 - val_loss: 1.8399 - val_sparse_categorical_accuracy: 0.4075\n","Epoch 15/15\n","2074/2074 [==============================] - 474s 229ms/step - loss: 1.6552 - sparse_categorical_accuracy: 0.4170 - val_loss: 1.8280 - val_sparse_categorical_accuracy: 0.4114\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fc4aa5dead0>"]},"metadata":{},"execution_count":9}],"source":["# Model Training\n","pretraining_model.fit(\n","    pretrain_ds,  validation_data=pretrain_val_ds, epochs=PRETRAINING_EPOCHS,callbacks=[tensorboard_callback],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Dm17R1A6EeY","executionInfo":{"status":"ok","timestamp":1669047349146,"user_tz":-60,"elapsed":7599,"user":{"displayName":"NLP Master","userId":"02125616247832443062"}},"outputId":"1d61f6f6-79f4-49bf-a709-f0b9c2a3dd55"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 108). These functions will not be directly callable after loading.\n"]}],"source":["# Save this base model for further finetuning.\n","encoder_model.save(\"seq2seq_all\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cNFaPZF1u70"},"outputs":[],"source":["#Load Tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir=\"path_to_save_execution_information\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aBul9L7Z6A_"},"outputs":[],"source":["#Code to automatically stop the run time for Google Colab\n","import time\n","time.sleep(60)\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyMF3eY1How+MNAq9IOLpiIJ"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
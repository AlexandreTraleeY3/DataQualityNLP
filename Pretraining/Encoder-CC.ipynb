{"cells":[{"cell_type":"markdown","source":["# Encoder"],"metadata":{"id":"nbPN8U-1B88m"}},{"cell_type":"markdown","source":["## Preparing the Environment"],"metadata":{"id":"WJAl2JWVCBOZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94848,"status":"ok","timestamp":1664874983643,"user":{"displayName":"Alexandre Carey","userId":"09783148984484458479"},"user_tz":-120},"id":"Nu4tsMTWLW_Q","outputId":"cc1dda72-d9b7-40b3-a910-5deb61a3cb9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["#Google Colab - Drive Mounting\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xclscTQtKdur"},"outputs":[],"source":["#Install missing library keras-nlp\n","!pip install -q keras-nlp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_pjD7CFKusX"},"outputs":[],"source":["# Import the libraries\n","import os\n","import keras_nlp\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","source":["## Data Preprocessing and Parameters Initialization"],"metadata":{"id":"byKISRYLCVum"}},{"cell_type":"markdown","metadata":{"id":"jEC3QYMYKyvH"},"source":["Reference: https://keras.io/guides/keras_nlp/transformer_pretraining/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2301,"status":"ok","timestamp":1664787965055,"user":{"displayName":"Alexandre Carey","userId":"09783148984484458479"},"user_tz":-120},"id":"xheZ-OJdMeZn","outputId":"c01c2c8c-74fb-4414-fc9d-1e78590c657f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n","7446528/7439277 [==============================] - 1s 0us/step\n","7454720/7439277 [==============================] - 1s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\n","237568/231508 [==============================] - 0s 0us/step\n","245760/231508 [===============================] - 0s 0us/step\n"]}],"source":["# Download of the vocabulary from BERT: Bert-uncased\n","vocab_file = keras.utils.get_file(\n","    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",")\n","#Initialization of the Word Tokenizer, with a given vocabulary and a sequence length\n","tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n","    vocabulary=vocab_file, sequence_length=SEQ_LENGTH,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3K-tfChKxpY"},"outputs":[],"source":["# Preprocessing paramseters\n","PRETRAINING_BATCH_SIZE = 128\n","FINETUNING_BATCH_SIZE = 32\n","SEQ_LENGTH = 128\n","MASK_RATE = 0.25\n","PREDICTIONS_PER_SEQ = 32\n","\n","# Model paramerters\n","NUM_LAYERS = 3\n","MODEL_DIM = 256\n","INTERMEDIATE_DIM = 512\n","NUM_HEADS = 4\n","DROPOUT = 0.1\n","NORM_EPSILON = 1e-5\n","\n","# Training paramseters\n","PRETRAINING_LEARNING_RATE = 5e-4\n","PRETRAINING_EPOCHS = 8 #original 8 but might be less training sample\n","FINETUNING_LEARNING_RATE = 5e-5\n","FINETUNING_EPOCHS = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwltpTNvMi4Y"},"outputs":[],"source":["# Load the data\n","CC_train_ds = (\n","    tf.data.TextLineDataset('path_to_pretraining_train_data/train-all.csv')\n","    .filter(lambda x: tf.strings.length(x) > 100)\n","    .batch(PRETRAINING_BATCH_SIZE)\n",")\n","CC_val_ds = (\n","    tf.data.TextLineDataset('path_to_pretraining_test_data/test-all.csv')\n","    .filter(lambda x: tf.strings.length(x) > 100)\n","    .batch(PRETRAINING_BATCH_SIZE)\n",")"]},{"cell_type":"markdown","metadata":{"id":"D9Ok4kx7NcIH"},"source":["## Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_E1cfGwMK_v"},"outputs":[],"source":["def preprocess(inputs):\n","    \"\"\"\n","    Given a text input, \n","    return a set of features containing the masked words, \n","    the true labels of these masked word and their weights\n","    \"\"\"\n","    inputs = tokenizer(inputs)\n","    outputs = masker(inputs)\n","    features = {\n","        \"tokens\": outputs[\"tokens\"],\n","        \"mask_positions\": outputs[\"mask_positions\"],\n","    }\n","    labels = outputs[\"mask_ids\"]\n","    weights = outputs[\"mask_weights\"]\n","    return features, labels, weights\n","\n","# Create the Masker object\n","masker = keras_nlp.layers.MLMMaskGenerator(\n","    vocabulary_size=tokenizer.vocabulary_size(),\n","    mask_selection_rate=MASK_RATE,\n","    mask_selection_length=PREDICTIONS_PER_SEQ,\n","    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",")\n","\n","# Preprocess the data\n","pretrain_ds = CC_train_ds.map(\n","    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",").prefetch(tf.data.AUTOTUNE)\n","pretrain_val_ds = CC_val_ds.map(\n","    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",").prefetch(tf.data.AUTOTUNE)\n","\n","# Display a single element of the dataset\n","print(pretrain_ds.take(1).get_single_element())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":897,"status":"ok","timestamp":1664788046516,"user":{"displayName":"Alexandre Carey","userId":"09783148984484458479"},"user_tz":-120},"id":"8UlD_x-GNjP2","outputId":"510ad0e8-2d5a-4c1a-9ba0-3dd5b6308372"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 128)]             0         \n","                                                                 \n"," token_and_position_embeddin  (None, 128, 256)         7846400   \n"," g (TokenAndPositionEmbeddin                                     \n"," g)                                                              \n","                                                                 \n"," layer_normalization (LayerN  (None, 128, 256)         512       \n"," ormalization)                                                   \n","                                                                 \n"," dropout (Dropout)           (None, 128, 256)          0         \n","                                                                 \n"," transformer_encoder (Transf  (None, 128, 256)         527104    \n"," ormerEncoder)                                                   \n","                                                                 \n"," transformer_encoder_1 (Tran  (None, 128, 256)         527104    \n"," sformerEncoder)                                                 \n","                                                                 \n"," transformer_encoder_2 (Tran  (None, 128, 256)         527104    \n"," sformerEncoder)                                                 \n","                                                                 \n","=================================================================\n","Total params: 9,428,224\n","Trainable params: 9,428,224\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Define input type\n","inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n","\n","# Embdding layer\n","embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n","    vocabulary_size=tokenizer.vocabulary_size(),\n","    sequence_length=SEQ_LENGTH,\n","    embedding_dim=MODEL_DIM,\n",")\n","outputs = embedding_layer(inputs)\n","\n","# Normalization and Dropout\n","outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n","outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n","\n","# Create encoder blocks\n","for i in range(NUM_LAYERS):\n","    outputs = keras_nlp.layers.TransformerEncoder(\n","        intermediate_dim=INTERMEDIATE_DIM,\n","        num_heads=NUM_HEADS,\n","        dropout=DROPOUT,\n","        layer_norm_epsilon=NORM_EPSILON,\n","    )(outputs)\n","\n","#Build model and display\n","encoder_model = keras.Model(inputs, outputs)\n","encoder_model.summary()"]},{"cell_type":"markdown","source":["## Model Training"],"metadata":{"id":"c3NB-d7jDbV3"}},{"cell_type":"code","source":["# Adding masked language model head\n","inputs = {\n","    \"tokens\": keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32),\n","    \"mask_positions\": keras.Input(shape=(PREDICTIONS_PER_SEQ,), dtype=tf.int32),\n","}\n","\n","#Create tensorboard callback\n","logdir = \"path_to_save_execution_information\" #+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n","\n","# Encode the input tokens\n","encoded_tokens = encoder_model(inputs[\"tokens\"])\n","\n","# Predict output for each masked word\n","outputs = keras_nlp.layers.MLMHead(\n","    embedding_weights=embedding_layer.token_embedding.embeddings, activation=\"softmax\",\n",")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n","\n","# Compile our model\n","pretraining_model = keras.Model(inputs, outputs)\n","pretraining_model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=PRETRAINING_LEARNING_RATE),\n","    weighted_metrics=[\"sparse_categorical_accuracy\"],\n","    jit_compile=True,\n",")"],"metadata":{"id":"kYb7S9YdDg5S"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"PJFkKQYDNjni","outputId":"50bf96d7-a150-436d-8324-b90ea6eeaca3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/8\n","47029/47029 [==============================] - 7494s 159ms/step - loss: 1.8379 - sparse_categorical_accuracy: 0.3348 - val_loss: 1.5893 - val_sparse_categorical_accuracy: 0.3939\n","Epoch 2/8\n","47029/47029 [==============================] - 7432s 158ms/step - loss: 1.6178 - sparse_categorical_accuracy: 0.3833 - val_loss: 1.5207 - val_sparse_categorical_accuracy: 0.4098\n","Epoch 3/8\n","47029/47029 [==============================] - 7434s 158ms/step - loss: 1.5780 - sparse_categorical_accuracy: 0.3920 - val_loss: 1.4936 - val_sparse_categorical_accuracy: 0.4163\n","Epoch 4/8\n","47029/47029 [==============================] - 7402s 157ms/step - loss: 1.5584 - sparse_categorical_accuracy: 0.3963 - val_loss: 1.4805 - val_sparse_categorical_accuracy: 0.4192\n","Epoch 5/8\n","47029/47029 [==============================] - 7427s 158ms/step - loss: 1.5463 - sparse_categorical_accuracy: 0.3989 - val_loss: 1.4701 - val_sparse_categorical_accuracy: 0.4215\n","Epoch 6/8\n","47029/47029 [==============================] - 7426s 158ms/step - loss: 1.5383 - sparse_categorical_accuracy: 0.4008 - val_loss: 1.4625 - val_sparse_categorical_accuracy: 0.4237\n","Epoch 7/8\n","47029/47029 [==============================] - 7400s 157ms/step - loss: 1.5321 - sparse_categorical_accuracy: 0.4021 - val_loss: 1.4561 - val_sparse_categorical_accuracy: 0.4248\n","Epoch 8/8\n","47029/47029 [==============================] - 7391s 157ms/step - loss: 1.5276 - sparse_categorical_accuracy: 0.4031 - val_loss: 1.4541 - val_sparse_categorical_accuracy: 0.4256\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f9dd95fe610>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Model Training\n","pretraining_model.fit(\n","    pretrain_ds,  validation_data=pretrain_val_ds, epochs=PRETRAINING_EPOCHS,callbacks=[tensorboard_callback],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O2DrAf1sNs_S","outputId":"5dc6ad77-e93a-470b-bc27-70966b4d3e84"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 82). These functions will not be directly callable after loading.\n"]}],"source":["# Save this base model for further finetuning.\n","encoder_model.save(\"encoder_all\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":821},"id":"q9a-s-y2r3Gd","executionInfo":{"status":"ok","timestamp":1664874997356,"user_tz":-120,"elapsed":8802,"user":{"displayName":"Alexandre Carey","userId":"09783148984484458479"}},"outputId":"8e3cacdf-c888-4460-ce8e-3c27f1a9a03b"},"outputs":[],"source":["#Load Tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir=\"path_to_save_execution_information\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BKnmYEeq5-pt"},"outputs":[],"source":["#Code to automatically stop the run time for Google Colab\n","import time\n","time.sleep(60)\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}